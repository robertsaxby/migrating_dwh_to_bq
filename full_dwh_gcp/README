use case 2a: DWH completely migrated to GCP !
setup/0_bq_external_create.sh -> create external tables on OLTP data - staging area GCS
setup/1_bq_create.sh -> create tables in BQ for the external data -> not really used in the demo, but can be used to highlight the fact that BQ can infacy be a staging area for RAW data partitioned by day !
setup/schema -> schema used by external tables script to created federated tables in BQ

elt/dags/da304-full-dwh-gcp-daily.py -> ELT to load and tranbform OLTP data into a star schema within BQ
elt/dags/query/broker.sql -> join of broker table and status table - dimension
elt/dags/query/company.sql -> join of company table, address, industry and status table - dimension
elt/dags/query/customer_account.sql -> join of customer_account, customer, address, status tables. - dimension
elt/dags/query/security.sql -> join of security, company, exchange, status, address tables - dimension
elt/dags/query/trade.sql -> join of trade , trade_type and status tables - fact

The SQL's are executed by the elt/dags/da304-full-dwh-gcp-daily.py DAG !

use case 2b: Stream data into BQ, DML capabilities 
stream/data/stream.py -> python script to stream Trade data into BQ trade table. reads a file and loops
stream/setup/create_trade_streaming_table.sh -> create streaming table "Trade" in BQ
stream/1_before_wrong_upate.sh -> BQ query to show curremt state of table
stream/2_wrong_update.sh -> BQ query to perform incorrect update (update of all rows belonging to a customer instead of customer and symbol ! )
stream/3_fix_erroneous_update.sh -> series of BQ queries to fetch data from BQ snapshot to recover from erroneous update and performing the correct update !

